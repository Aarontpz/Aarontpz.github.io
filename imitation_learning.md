Imitation learning seeks to take samples of actions in a given state (the actions are typically from “experts”, imagine learning the ideal move in a game of chess based on what Bobby Fischer did in a similar state), without no clear reward signal. Many reinforcement-learning methods rely on the reward signal, but many more tasks which are interesting (namely, almost any real-world task) does not provide the reward signal directly, which indicates that imitation learning may be a more robust approach to reinforcement learning than reinforcement learning methods relying on the signal. <br/><br/>
    A potential (but naive) solution to the imitation learning problem using GANs would involve using a GAN to assist the task of inverse-reinforcement learning DETAIL INVERSE RL HERE, in order to derive a reward function from expert actions based on either very little information on the reward signal, or none at all. If a GAN could generate expert trajectories in a way which closely resemble the samples enough that the inverse-RL operating on the expert samples and generated samples moves towards the global optimum. The generated samples would need to resemble the expert trajectories close enough that the algorithm ALGORITHM HERE does not move in a direction away from the “true optima” of the space that the trajectories make up. (as per our previous definition of “enough”) <br/><br/>
    More interestingly, however, is to forgo the idea of retrieving the reward signal entirely, and instead focus on making the most of the expert actions given. 
OpenAI released a paper titled “One-Shot Imitation Learning” which details a neural network architecture for extracting information about a single “demonstration” of a task, and from that single demonstration modifies the policy function to optimally perform for a similar task. The implementation involves three networks working in conjunction - a demonstration network, which takes the time-series demonstration data and produces some variable-size embedding of it, a context network which takes both the current state of the model and the demonstration embedding and produces a fixed-length “context embedding”, which must (in order to solve the problem) contain only the relevant information to complete the task. This embedding is finally passed on to the manipulator network which actually controls the actuator in order to solve its current task. 
The results OpenAI got by applying this concept in the context of a “block-stacking” environment, where a robotic arm must stack colored blocks positioned randomly in an environment in a particular order based only on demonstrations of the task being done (the demonstrations providing all of the context regarding the order of the colors in the stack) indicate that this approach outperforms attempts to extrapolate value signals from the state-action pairs produced in the demonstration. <br/><br/>
There are several ways in which adding a GAN structure to the One-Shot Imitation Learning model could benefit it. The demonstration and context networks both use soft-attention to reduce very large time-series data into chunks of data containing only relevant pieces of information (DETAIL NEURAL ATTENTION / SOFT-ATTENTION HERE). It has been shown that generative adversarial models are quite efficient at generating hard (or soft) attention masks to benefit their tasks [6], and this conveniently is the most difficult part of the demonstration and context networks! Thus, it stands to reason that having a GAN or GAN-like network generating attention masks as if they were being generated as part of the demonstration / context network could reduce the actual training time of those networks. <br/><br/>
Additionally, the demonstration, context, and manipulator networks all rely on a large number of demonstrations being present to train the system to accurately extract the necessary data from the demonstrations and current-state in order to achieve a task. In this was, one-shot imitation learning can be susceptible to sparsity in the demonstrations - especially when dealing with real-world tasks where demonstrations would, in theory, require a person to perform some task from different starting and ending conditions many times over in order to adequately capture the nature of the task for the network to learn. GANs generating sequences over time is a fairly new field (I WOULD LIKE TO HAVE SOMETHING TO CONTRIBUTE HERE), but if a generative model could adequately capture the distribution of a set of demonstrations over time, with even the slightest bit of accuracy, it could greatly speed up the training of the one-shot imitation learner, or at least compensate for a general lack of examples. (Additionally, a conditional-GAN generating observation-action pairs could effectively be performing imitation learning, look into this?)    <br/><br/>
    Another approach to getting around the inefficiencies of inverse reinforcement learning and behavioral cloning is through an algorithm called generative adversarial imitation learning (GAIL) (SOURCE SOURCE). GAIL seeks to bypass the “extra step” that inverse reinforcement learning processes add; instead of going from expert trajectories -> IRL-derived value function -> optimal policy, why not go simply from expert trajectories to optimal policy. This may seem like a familiar question - going from expert trajectories to optimal policies is the goal of basically all imitation-learning approaches, but GAIL takes a different approach than the previously mentioned methods: a discriminator D, like the discriminator in a GAN, attempts to distinguish between a generator G’s output and the “true data” G is attempting to emulate. However, this task-specific GAN’s generator is a policy which attempts to match the “occupancy measure” of the trajectory distribution, which is defined as the distribution of state-action pairs that the policy encounters.[4] <br/><br/>
    This sounds like a restating of previous thoughts in this series: having a GAN in which the generator generates actions has been a recurring theme. However, the brilliance of this approach lies in how the problem is restated - instead of the discriminator operating directly on an action space, it can operate on a constantly-updated approximation of the occupancy measure, which more completely represents the generator / expert policy as a whole than a finite set of state-action pairs. The paper argues that while approximately optimal policies for a given environment are not necessarily unique, each policy has a unique measure of occupancy. <br/>
To understand this, consider a game of checkers: a parameterized policy which favors moving pieces to the left or rightmost side of the board will have a probability distribution of state-action pairs favoring states and actions based on this bias (as in, it will be much more likely for the policy to generate a state-action pair moving a piece to the rightmost side of the board than towards the center). If this inefficient policy were compared to a checkers master, who favors state-action pairs that protect their pieces and cut off enemy moves, the distribution of state-action pairs would be quite different. Wouldn’t it be nice to teach the side-obsessed policy to behave like the master? It would be difficult to simply use the difference in occupancy-measures between the policies as a loss-function (the occupancy measure function described in the paper is BETTER EXPLAIN OCUCPANCY FUNCTION), since the occupancy measure is a function of incredibly high-dimension data, and thus the variance when training the policy would be quite high. Of course, this is why using a GAN-like approach makes sense: the discriminator can act as the loss function between the policies, forcing the generator to minimize the difference between it and the expert’s occupancy measure in order to fool the discriminator. <br/>
This is simply a few examples of how GANs and imitation learning problems benefit one another. 

