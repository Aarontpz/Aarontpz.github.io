Imitation learning seeks to take samples of actions in a given state (the actions are typically from “experts”, imagine learning the ideal move in a game of chess based on what Bobby Fischer did in a similar state), without no clear reward signal. Many reinforcement-learning methods rely on the reward signal, but many more tasks which are interesting (namely, almost any real-world task) does not provide the reward signal directly, which indicates that imitation learning may be a more robust approach to reinforcement learning than reinforcement learning methods relying on the signal. 

A potential (but naive) solution to the imitation learning problem using GANs would involve using a GAN to assist the task of [inverse-reinforcement learning](https://people.eecs.berkeley.edu/~pabbeel/cs287-fa12/slides/inverseRL.pdf), which seeks to derive a reward function from expert actions based on either very little information on the reward signal, or none at all. The logic behind inverse reinforcement learning (IRL) is that, if a value function can be found using expert trajectories, an optimal policy can be derived through conventional reinforcement learning methods, using that value function. If a GAN could generate expert trajectories in a way which closely resemble the expert samples, it would alleviate the problem of not having enough expert trajectories to conventionally apply IRL. This naive application of GANs to a pre-existing reinforcement learning method is, it should be obvious, a theme of this blog series. 
    
More interestingly, however, is to forgo the idea of retrieving the reward signal entirely, and instead focus on making the most of the expert actions given. 
OpenAI released a paper titled “[One-Shot Imitation Learning](https://arxiv.org/pdf/1703.07326.pdf)” which details a neural network architecture for extracting information about a single “demonstration” of a task, and from that single demonstration modifies the policy function to optimally perform for a similar task. They demonstrate this by creating environments in which the overarching task remains consistent, but nuances change: for example, a task of stacking different colored blocks on each other. The overarching objective (stack blocks) remains the same, but the target color-sequence can be changed, and the imitation learner must adapt to this change.

The implementation involves three networks working in conjunction - a demonstration network, which takes the time-series demonstration data and produces some variable-size embedding of it, a context network which takes both the current state of the model and the demonstration embedding and produces a fixed-length “context embedding”, which must (in order to solve the problem) contain only the relevant information to complete the task. This embedding is finally passed on to the manipulator network which actually controls the actuator in order to solve its current task. 
The results OpenAI got by applying this concept in the context of the “block-stacking” environment indicates that this approach outperforms attempts to extrapolate value signals from the state-action pairs produced in the demonstration. 

There are several ways in which adding a GAN structure to the One-Shot Imitation Learning model could benefit it. The demonstration and context networks both use soft-attention to reduce very large time-series data into chunks of data containing only relevant pieces of information. Attention in neural networks is basically the application of a mask to an input field (like an image) to focus on particular aspects. When dealing with images, the mask is typically the same size as the input image, and is multiplied element-wise with the image. The mask in a hard-attention mechanism can only take on the values 0 or 1, where in soft-attention the mask can take on a discrete or continuous range between 0 and 1. Essentially, the mask provides a means for the network on focusing on some part of a field, typically a "part" of the field which matches some [feature the network has been trained to recognize, like rain drops in an image](https://www.arxiv-vanity.com/papers/1711.10098/). 

It has been shown that generative adversarial models are quite capable of generating hard (or soft) attention masks to benefit their tasks (as explored and described in [this paper](https://www.arxiv-vanity.com/papers/1711.10098/)), and learning to generate the right mask for a task is *conveniently* is the most difficult part of the demonstration and context networks! Since, as the previous post discussed, GANs excel at taking a conditioning image and generating an output image (like, say, a hard or soft mask), this is, quite intuitively, a problem where a GAN could potentially improve or speed up the performance of the network.

Additionally, the demonstration, context, and manipulator networks in "One Shot Imitation Learning" all rely on a large number of demonstrations being available to train the system to extract the necessary data from the demonstrations and relate this data to its current task and current set of observations. In this way, one-shot imitation learning can be susceptible to sparsity in the demonstrations - especially when dealing with real-world tasks where demonstrations would, in theory, require a person to perform some task from different starting and ending conditions many times over in order to adequately capture the nature of the task for the network to learn. GANs generating sequences over time is a fairly new field (which I would love to eventually contribute to), but if a generative model could adequately capture the distribution of a set of demonstrations over time, with even the slightest bit of accuracy, it could greatly speed up the training of the one-shot imitation learner, or at least compensate for a general lack of examples. Interestingly, having the GAN generate state-action pairs effectively makes it behave like the next algorithm, generative adversarial imitation learning.

Another approach to getting around the inefficiencies of inverse reinforcement learning and behavioral cloning to perform imitation learning is through an algorithm called [generative adversarial imitation learning](https://arxiv.org/pdf/1606.03476.pdf) (GAIL). GAIL seeks to bypass the intermediary step of performing inverse reinforcement learning to eventually find an optimal policy; instead of going from expert trajectories to an IRL-derived value function to an optimal policy, it would be more efficient to go simply from expert trajectories to optimal policy. GAIL takes a different approach than the previously mentioned methods: a discriminator D, like the discriminator in a GAN, attempts to distinguish between a generator G’s output and the “true data” G is attempting to emulate. However, this task-specific GAN’s generator is a policy which attempts to match the “occupancy measure” of the trajectory distribution, which is defined as the distribution of state-action pairs that the policy encounters.

This may sound like a restating of previous thoughts in this series: having a GAN in which the generator generates actions has been a recurring theme. However, the brilliance of this approach lies in how the problem is restated - instead of the discriminator operating directly on an action space, it can operate on a constantly-updated approximation of the occupancy measure, which more completely represents the generator / expert policy as a whole than a finite set of state-action pairs. The paper argues that while approximately optimal policies for a given environment are not necessarily unique, each policy has a unique measure of occupancy. 

To understand this, consider a game of checkers: a parameterized policy which favors moving pieces to the left or rightmost side of the board will have a probability distribution of state-action pairs favoring states and actions based on this bias (as in, it will be much more likely for the policy to generate a state-action pair moving a piece to the rightmost side of the board than towards the center). If this inefficient policy were compared to a checkers master, who favors state-action pairs that protect their pieces and cut off enemy moves, the distribution of state-action pairs would be quite different. Wouldn’t it be nice to teach the side-obsessed policy to behave like the master? It would be difficult to simply use the difference in occupancy-measures between the policies as a loss-function, since the occupancy measure is a function of incredibly high-dimension data, and thus the variance when training the policy would be quite high. Of course, this is why using a GAN-like approach makes sense: the discriminator can act as the loss function between the policies, forcing the generator to minimize the difference between it and the expert’s occupancy measure in order to fool the discriminator. 

This is simply a few examples of how GANs and imitation learning problems benefit one another. 

