Actor-critic models, and the more popular [Asynchronous-Advantage-Actor-Critic](https://arxiv.org/pdf/1602.01783.pdf) model, both approximate a value function in order to better optimize a policy (both of which are parameterized through neural networks). The simpler Actor-Critic model optimizes its policy by taking the gradient of the likelihood of choosing an action in a given state (which is the output of the policy) times an "advantage" value, which represents the difference between the actual overall reward for choosing an action in a state and the estimated value of simply being in that state. These methods are considered extremely powerful and cutting edge, combining the power of deep q-learning and policy gradient methods.

Interestingly, some papers (such as [this paper, connecting actor-critic models to GANs](https://arxiv.org/pdf/1610.01945.pdf)) have suggested that there is an intimate relationship between actor-critic models and GANs. Namely, the critic in the AC (or advantage-actor critic, or A3C) model is very similar to the discriminator in a GAN, in that their outputs inform either part (for the critic) or all (for the discriminator) of the gradient that their counterpart updates based off of. The main difference between the adversary and the critic comes from their objectives - the critic seeks to accurately model some (scalar) value function V(s), whereas the discriminator attempts to accurately discern whether or not a sample came from a dataset (and outputs a binary value representing whether or not it believes this). The objective of the discriminator is suited towards maximizing the expected number of dataset samples get “approved” and minimizing the expected number of generator samples which get “approved”, whereas the value function simply attempts to map a state to a value representing the expected return from being in that state. 

An interesting (and important) similarity between actor-critic models and GANs is the approach taken to get each model to converge; it is well-documented that GANs are very difficult to force to converge, since each participant in the two-player game that they represent is, to put it colloquially, chasing a moving target. The discriminator is constantly getting better at detecting generator samples, which are simultaneously being refined to fool the discriminator. This is somewhat analogous to issues that commonly plague reinforcement learning models (and actor-critic models are no exception) where the sample variance is too small, leading the model to “overfit” on states/actions it has seen most recently. A solution which addresses both of these problems, which seems intuitively sound, is the idea of “replay buffers” actor-critic models. Simply put, a replay buffer keeps a record of the pertinent environment details in some series of decisions made by the actor (st, at, rt, st+1) and enables the actor and critic to train on those recorded samples instead of relying on the “most recent” samples from the environment (this is detailed in the aforementioned paper which links GANs and AC-models). This concept can be applied to GANs in several ways: for starters, storing generator samples and periodically sampling them as inputs to the discriminator can keep the discriminator from overfitting and dominating the generator in its current state. A more interesting application could be keeping a record of previous configurations of the generator / discriminator (i.e. storing weights) and occasionally using these past configurations in order to keep the sample variance the GAN is working with high enough to prevent overfitting or converging to suboptimal solutions. This has been proposed, alongside [wasserstein GAN](https://www.alexirpan.com/2017/02/22/wasserstein-gan.html) to keep the GAN training process from experiencing mode collapse, where either the generator or (typically) the discriminator become far too powerful for the other to "catch up". 


