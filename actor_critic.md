Actor-critic models, and the more popular Asynchronous-Advantage-Actor-Critic model, both rely on approximating a value function in order to better optimize a policy, based on the gradient DETAILS ON AC/Advantage-actor-critic HERE. <br/><br/>
Interestingly, some papers [2] have suggested that there is an intimate relationship between actor-critic models and GANs. Namely, the critic in the AC (or advantage-actor critic, or A3C) model is very similar to the discriminator in a GAN, in that their outputs inform either part (for the critic) or all (for the discriminator) of the gradient that their counterpart updates based off of. The main difference between the adversary and the critic comes from their objectives - the critic seeks to accurately model some (scalar) value function V(s), whereas the discriminator attempts to accurately discern whether or not a sample came from a dataset (and outputs a binary value representing whether or not it believes this). The objective of the discriminator is suited towards maximizing the expected number of dataset samples get “approved” and minimizing the expected number of generator samples which get “approved”, whereas the value function simply attempts to map a state to a value representing the expected return from being in that state. <br/><br/>
An interesting (and important) similarity between actor-critic models and GANs is the approach taken to get each model to converge; it is well-documented that GANs are very difficult to force to converge, since each participant in the two-player game that they represent is, to put it colloquially, chasing a moving target. This is somewhat analogous to issues that commonly plague reinforcement learning models (and actor-critic models are no exception) where the sample variance is too small, leading the model to “overfit” on states/actions it has seen most recently. A solution which addresses both of these problems, which seems intuitively sound, is the idea of “replay buffers” actor-critic models. Simply put, a replay buffer keeps a record of the pertinent environment details in some series of decisions made by the actor (st, at, rt, st+1) and enables the actor and critic to train on those recorded samples instead of relying on the “most recent” samples from the environment.[2] This concept can be applied to GANs in several ways: for starters, storing generator samples and periodically sampling them as inputs to the discriminator can keep the discriminator from overfitting and dominating the generator in its current state. A more interesting application could be keeping a record of previous configurations of the generator / discriminator (i.e. storing weights) and occasionally using these past configurations in order to keep the sample variance the GAN is working with high enough to prevent overfitting or converging to suboptimal solutions.<br/><br/>

