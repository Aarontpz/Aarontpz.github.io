Both supervised and reinforcement learning methods suffer from issues of sample scarcity - a lack of samples to properly represent or capture the target distribution. A good example of this is the task of learning how to play chess: how many times would you have to see chess be played to understand its core mechanics, with no prior knowledge? What value would you assign to each move on the chessboard? How many hundreds of games would you have to observe to find relationships between states and optimal actions? Would those hundreds of games even begin to cover the full range of possible states and actions in a way which adequately demonstrates the optimal value of an action in a given state? 

![Decision tree for a single turn in Chess. Source: https://kevinbinz.com/2015/02/26/decision-trees-in-chess/](https://kevinbinz.files.wordpress.com/2015/02/chess-decision-tree-quiet-position-comparison1.png)
*Above: Possible decisions for a single turn in Chess.*

These are questions which normal humans pay very little attention to - a child can learn the rules of chess, and movements the average player makes can be based on search trees no more than three steps into the future (and is typically governed by intuition rather than a formal state-action pair value for each possible action in a given state). But for a parameterized model like a neural network, intuition cannot be explicitly coded (yet), and formalizing the optimal solution for a given state almost certainly requires consideration of many possible states far into the future. 

Certainly there exist optimal solutions to the game of chess - the outcome of the game is closely enough related to a sequence of a few to a dozen state-action pairs that any sufficiently powerful model would be able to learn optimal policies fairly efficiently (and, to be sure, chess-playing algorithms [have been consistently beating human champions for quite some time now] (https://thebestschools.org/magazine/brief-history-of-computer-chess/)). Chess is a perfect-information game, in which each player can, by looking at the current state of the game, extract all of the information necessary to inform their next decision. In reinforcement learning problems Chess would be considered a __deterministic__ (each step can be traced back to the starting position, as opposed to a __stochastic__ process where a level of uncertainty and chance affects state transitions) and __Markovian__ process (meaning the optimal action from a given state can be decided [without information on previous actions/states](https://www.cs.rice.edu/~vardi/dag01/givan1.pdf)).

However, in stochastic or non-Markovian environments (that is, in environments where the state information is not fully encapsulated in a n-length series of observations), the above problems are much more difficult to work around. Stochastic environments like video games, where random number generators are employed to effect certain environmental variables, cannot have their future states parsed by conventional tree-search algorithms. Environments with imperfect information, like stratego or video games with “off-screen” events, likewise cannot be normally dealt with by conventional tree search algorithms. 

GAN networks, which excel at capturing distributions from sparse samples, may be able to alleviate this problem. Consider the problem of sample scarcity in the Atari reinforcement learning environment (introduce Atari RL environment, relate all algorithms to it, mention neural game engine idea?) - in a series of ten or even a hundred games of, for example, Tetris, a naive reinforcement learning algorithm such as Q-learning may still not encounter particular states which require a particular action / series of actions to navigate. However, if a GAN could learn to capture the game environment as its distribution (perhaps over time, but simply generating a single valid state in the environment would be beneficial), and have some weak approximation of how an action in the generated state effects the following states and reward signals, then the GAN could enable the reinforcement learner to sample the environment in such a way that it does not have to deal with overfitting on starting states (Atari games, and “games” in general like chess, all start from the same state, and thus the first n-states are sampled much more frequently than latter states). 

This concept can be further applied through the idea of conditional GANs - specifically, GAN networks which take some distribution (such as an image) and transform it into another distribution (such as another image of a different kind). Pix2pix is the most popular implementation of this concept [1], and focuses on encoding important features of an input image and producing some transformation on that image in order to achieve another image (such as taking “edges” or drawings and converting them to cats SOURCE). Such a model has been shown to successfully be applied to (SOURCE SOURCE SOURCE). Since game environments can necessarily be encoded into information for a neural network (otherwise all reinforcement learning methods would fail, since there would be no way to extract information from a state), a “pix2pix”-esque approach to generating much information from an observation in a reinforcement learning environment is quite possible: given a state-action pair as conditioning values (SHOULD WE ELABORATE ON PIX2PIX?) it would be quite reasonable to receive the next “most likely” state as the output of the GAN, given a sufficient collection of samples from the environment (or pre-training the network on a similar environment and then updating the weights for the particular task). This would allow a much more educated approach to estimating the next best action, as much more information would be available - even if the “next state” was synthesized by an approximative model. 

